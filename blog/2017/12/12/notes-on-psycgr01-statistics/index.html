<!DOCTYPE html><html lang="en"><meta charset="utf-8" /><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>Notes on 'PSYCGR01: Statistics'</title><meta name="author" content="Jamie Brandon" /><link rel="alternate" type="application/rss+xml" title="Scattered Thoughts - " href="/feed.xml" /><style> @import url("https://fonts.googleapis.com/css?family=Fira+Code:400,700|Fira+Sans:400,400i,700,700i&display=swap");progress,sub,sup{vertical-align:baseline}button,hr,input{overflow:visible}html{font-family:sans-serif;line-height:1.15;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}body{margin:0}figcaption,menu,article,aside,details,figure,footer,header,main,nav,section,summary{display:block}audio,canvas,progress,video{display:inline-block}audio:not([controls]){display:none;height:0}[hidden],template{display:none}a{background-color:transparent;-webkit-text-decoration-skip:objects}a:active,a:hover{outline-width:0}abbr[title]{border-bottom:none;text-decoration:underline;text-decoration:underline dotted}b,strong{font-weight:bolder}dfn{font-style:italic}h1{font-size:2em;margin:.67em 0}mark{background-color:#ff0;color:#000}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}svg:not(:root){overflow:hidden}code,kbd,pre,samp{font-family:monospace,monospace;font-size:1em}figure{margin:1em 40px}hr{box-sizing:content-box;height:0}button,input,optgroup,select,textarea{font:inherit;margin:0}optgroup{font-weight:700}button,select{text-transform:none}[type=submit],[type=reset],button,html [type=button]{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:ButtonText dotted 1px}fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-cancel-button,[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-input-placeholder{color:inherit;opacity:.54}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}code{background:#ffffff}.highlight{background:#ffffff}.highlight pre{background-color:#fff;font-size:16px}.highlight .c{color:#999988;font-style:italic}.highlight .err{color:#a61717;background-color:#e3d2d2}.highlight .k{font-weight:bold}.highlight .o{font-weight:bold}.highlight .cm{color:#999988;font-style:italic}.highlight .cp{color:#999999;font-weight:bold}.highlight .c1{color:#999988;font-style:italic}.highlight .cs{color:#999999;font-weight:bold;font-style:italic}.highlight .gd{color:#000000;background-color:#fdd}.highlight .gd .x{color:#000000;background-color:#faa}.highlight .ge{font-style:italic}.highlight .gr{color:#a00}.highlight .gh{color:#999}.highlight .gi{color:#000000;background-color:#dfd}.highlight .gi .x{color:#000000;background-color:#afa}.highlight .go{color:#888}.highlight .gp{color:#555}.highlight .gs{font-weight:bold}.highlight .gu{color:#aaa}.highlight .gt{color:#a00}.highlight .kc{font-weight:bold}.highlight .kd{font-weight:bold}.highlight .kp{font-weight:bold}.highlight .kr{font-weight:bold}.highlight .kt{color:#445588;font-weight:bold}.highlight .m{color:#099}.highlight .s{color:#d14}.highlight .na{color:teal}.highlight .nb{color:#0086B3}.highlight .nc{color:#445588;font-weight:bold}.highlight .no{color:teal}.highlight .ni{color:purple}.highlight .ne{color:#990000;font-weight:bold}.highlight .nf{color:#990000;font-weight:bold}.highlight .nn{color:#555}.highlight .nt{color:navy}.highlight .nv{color:teal}.highlight .ow{font-weight:bold}.highlight .w{color:#bbb}.highlight .mf{color:#099}.highlight .mh{color:#099}.highlight .mi{color:#099}.highlight .mo{color:#099}.highlight .sb{color:#d14}.highlight .sc{color:#d14}.highlight .sd{color:#d14}.highlight .s2{color:#d14}.highlight .se{color:#d14}.highlight .sh{color:#d14}.highlight .si{color:#d14}.highlight .sx{color:#d14}.highlight .sr{color:#009926}.highlight .s1{color:#d14}.highlight .ss{color:#990073}.highlight .bp{color:#999}.highlight .vc{color:teal}.highlight .vg{color:teal}.highlight .vi{color:teal}.highlight .il{color:#099}.highlight .lineno{color:rgba(0,0,0,0.3);padding:0 10px;-webkit-user-select:none;-moz-user-select:none;-o-user-select:none}.lineno::-moz-selection{background-color:transparent}.lineno::selection{background-color:transparent}body{padding:32px;color:#333333;background-color:#ffffff;font-family:Fira Sans, serif}.container{max-width:45em;margin:0 auto;font-size:20px}body blockquote{border-left:2px solid #333333 !important}article{font-size:1em}h1,h2,h3{font-weight:800;font-family:Fira Sans, sans-serif}h1{text-align:center;font-size:2.0em}h2{text-align:center;font-size:1.2em;margin-top:4em}h3{text-align:center;font-size:1em}h4{text-align:center}a{text-decoration:underline;font-weight:normal}a,a:visited,a:hover,a:active{color:#0085a1}*{max-width:100%}pre,figure,.wp-caption{margin:0px -10px 20px -10px;padding:0px 10px 0px 10px}blockquote{margin:0;padding:0px 10px 0px 10px;border-radius:5px}p>img:only-child,p>a:only-child>img:only-child,.wp-caption img,figure img{display:block}img{margin-left:auto;margin-right:auto}.caption,.wp-caption-text,figcaption{font-size:0.9em;line-height:1.48em;font-style:italic}code,pre{white-space:pre;overflow:visible;font-family:Fira Code, monospace}ul,ol{padding:0}ul{padding-left:30px;list-style:disc}ol{padding-left:30px;list-style:decimal}.post-link{padding-bottom:10px;text-align:center}.post-link a{text-decoration:none;color:#333333}.post-link a:focus,.post-link a:hover{color:#0085a1}.post-link .post-title{margin:0;font-size:18px}nav{text-align:center}nav a{font-size:1.4em}nav a,nav a:visited{text-decoration:none;color:#333333}nav a:focus,nav a:hover{color:#0085a1}.menu ul{list-style:none;padding:0;margin:0}header{margin:2em 0 2em 0;text-align:center}header h1{margin:0}footer{margin-top:4em;text-align:center;font-style:italic}iframe{width:560px;height:315px;display:block;margin:0 auto;padding-top:1em}table{margin-left:auto;margin-right:auto;border-collapse:collapse}table,th,td{padding:0.5em;border:0.5px solid #333333}hr{width:7em;margin-top:3em;margin-bottom:3em;border:0;height:1px;background-image:linear-gradient(to right, transparent, rgba(0,0,0,0.75), transparent)}</style><nav> <a href="/"> JAMIE BRANDON </a></nav><div class="container"><header><h1>Notes on 'PSYCGR01: Statistics'</h1></header><article role="main"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); </script> <script type="text/javascript" async="" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_SVG"> </script><p><a href="https://www.ucl.ac.uk/lifesciences-faculty-php/courses/viewcourse.php?coursecode=PSYCGR01">https://www.ucl.ac.uk/lifesciences-faculty-php/courses/viewcourse.php?coursecode=PSYCGR01</a><blockquote><p>This course provides a thorough introduction to the General Linear Model, which incorporates analyses such as multiple regression, ANOVA, ANCOVA, repeated-measures ANOVA. We will also cover extensions to linear mixed-effects models and logistic regression. All techniques will be discussed within a general framework of building and comparing statistical models. Practical experience in applying the methods will be developed through exercises with the statistics package SPSS.</blockquote><h2 id="lecture-1"><a href="https://moodle.ucl.ac.uk/course/view.php?id=11131">Lecture 1</a></h2><p>Ignore cookbook approach, do model comparison.<p>General linear model.<p>Inference as attempted generalization from sample to population (<strong>non-Bayesian?</strong>).<p>Want estimators to be:<ul><li>Unbiased - expected value is true value<li>Consistent - variance decreases as sample size increases<li>Efficient - smallest variance out of all unbiased estimators</ul><p>Efficient estimators:<ul><li>Count of errors -&gt; mode<li>Sum of absolute errors -&gt; median<li>Sum of squared errors -&gt; mean</ul><p>$MSE = \sum (Y_i - \hat{Y}_i)^2 / n - p$.<p><strong>TODO Why degrees of freedom?</strong><p>Review:<ul><li>What is inference?<li>Three desirable properties of estimators.</ul><p><a href="/classes/stats/Week1.ipynb">Exercises</a><h2 id="lecture-2"><a href="https://moodle.ucl.ac.uk/course/view.php?id=11131&amp;section=2">Lecture 2</a></h2><p>Model is model of population (<strong>which implies that we can include sampling method in inference if we think we can accurately model the bias</strong>).<p>Sum of squares reduced $SSR = \operatorname{SSE}(C) - \operatorname{SSE}(A)$<p>Proportional reduction in error $PRE = \frac{\operatorname{SSE}(C) - \operatorname{SSE}(A)}{\operatorname{SSE}(C)}$. On population is usually denoted $\eta^2$.<p>F-score for GLM: $F = \frac{\mathrm{PRE} / (\mathrm{PA} - \mathrm{PC})}{(1-\mathrm{PRE})/(n - \mathrm{PA})} \sim F(\mathrm{PA} - \mathrm{PC}, n - \mathrm{PA})$<p>F-test: reject null if $P_\mathrm{null}(F &gt; F_\mathrm{observed}) &lt; \alpha$. Fixes $P_\mathrm{null}(\mathrm{Type1}) = \alpha$. Produces tradeoff curve between $P_\mathrm{null}(\mathrm{Type2})$ and real effect size.<p>95% confidence interval of estimate = on 95% of samples, confidence interval falls around true population value = reject null if (1-$\alpha$) confidence interval does not contain null.<p>Review:<ul><li>Define f-score.<li>Define f-test.<li>Define confidence interval.</ul><p><a href="/classes/stats/Week2.ipynb">Exercises</a><h2 id="lecture-3"><a href="https://moodle.ucl.ac.uk/course/view.php?id=11131&amp;section=3">Lecture 3</a></h2><p>Multiple regression.<p>Test for unique effect of $X_i$ by comparing with model where $\beta_i=0$.<p>Omnibus test - testing multiple parameters at once. Prefer tests where $PA - PC = 1$ - easier to interpret success/failure.<p>$R^2$ - squared multiple correlation coefficient - ‘coefficient of determination’ - ‘proportion of variance explained’ - PRE of model over $Y_i = \beta_0 + \epsilon_i$.<p>$\eta^2$ - true value of PRE in population. Unbiased estimate $\hat{\eta}^2 = 1 - \frac{(1 - \mathrm{PRE})(n - \mathrm{PC})}{n - \mathrm{PA}}$.<p>Conventionally:<ul><li>Small effect $\eta^2=.03$<li>Medium effect $\eta^2=.13$<li>Large effect $\eta^2=.26$</ul><p>$1-\alpha$ confidence interval for slope $b_j \pm \sqrt{\frac{F_{1,n-p;\alpha}\mathrm{MSE}}{(n-1)S^2_{X_j}(1-R^2_j)}}$ where:<ul><li>$\mathrm{MSE} = \frac{\mathrm{SSE}}{n-p}$<li>Sample variance <script type="math/tex">S^2_{X_j} = \frac{\sum_{i=1}^n(X_j,i - \bar{X}_j)^2}{n-1}</script><li><script type="math/tex">R^2_j</script> is PRE of model <script type="math/tex">X_{j,i} = b_0 + \prod_{k \neq j} b_k X_{k,i} + e_i</script> vs model <script type="math/tex">X_{j,i}=b_0 + e_i</script> (proportion of variance of <script type="math/tex">X_j</script> that can be explained by other predictors)</ul><p>$(1 - R^2_j)$ also called tolerance - how uniquely useful is $X_j$<p>Model search:<ul><li>Enter - add variables in blocks<li>Forwards - start with best predictor, keep adding next best until PRE not significant<li>Backwards - start with all, keep removing worst until PRE becomes significant<li>Stepwise - forwards but may also remove parameters that fall beneath some threshold</ul><p>Better to rely on theory<p>Note, for null model $Y_i = b_0 + \epsilon $ we get $SSE = (n - 1)\operatorname{Var}(Y_i)$<p><a href="/classes/stats/Week3.ipynb">Exercises</a><h2 id="lecture-4"><a href="https://moodle.ucl.ac.uk/course/view.php?id=11131&amp;section=4">Lecture 4</a></h2><p>GLM assumptions:<ol><li>Normality - $\epsilon_i \sim Normal$<ul><li>Biased predictions</ul><li>Unbiasedness - $\epsilon_i$ has mean 0<ul><li>Biased test results</ul><li>Homoscedasticity - $\epsilon_i$ has constant variance (per i)<ul><li>Unbiased parameter estimates (<strong>?</strong>)<li>Biased test results</ul><li>Independence - $\epsilon_i$ are pairwise independent<ul><li>Model mis-specification</ul></ol><p>Histogram of residuals should be roughly normal (1).<p>Should be no relationship in residual vs predicted graph (2,3).<p>Quantile-quantile plot - $Y_i$ vs $Q_i$ where $Q_i$ s.t. $P(Y \leq Q_i) = \hat{p}_i \approx p(Y \leq Y_i)$ ie quantiles vs cdf of normal distribution. If $Y_i$ are normal than should be roughly straight.<p>Shapiro-Wilk or Kolmogorov-Smirnov tests for normality.<p>Breush-Pagan or Koenker or Levene test for homoscedasticity.<p>Randomized control or sequential dependence test for independence.<p>Transform dependent variables to achieve 1,3. Transform predictor to achieve 2.<p>Outlier detection:<ul><li>Mahalanobis distance - distance of data point from center<li>Leverage - weight of data point in parameter estimate<li>Studentized deleted residual - ?<li>Cook’s distance - does omission of a data point change model predictions</ul><p>Outlier tests run on all data points, so need multiple comparison correction.<p>Multicollinearity - as $R^2_j \xrightarrow 1$ the confidence interval $\xrightarrow \infty$. Detection:<ul><li>Tolerance or variance inflation factor<li>Correlation matrix</ul><p>Partial correlation between $Y$ and $X_i$ is $\operatorname{sign}(\beta_i) \sqrt{\operatorname{PRE}(M, M-X_i)} = \frac{\operatorname{PRE}(M, NULL) - \operatorname{PRE}(M - X_i, NULL)}{1 - \operatorname{PRE}(M - X_i, NULL)}$<p><a href="/classes/stats/Week4.ipynb">Exercises</a><h2 id="lecture-5"><a href="https://moodle.ucl.ac.uk/course/view.php?id=11131&amp;section=5">Lecture 5</a></h2><p>Moderation<ul><li>Effect of $X_1$ varies depending on value of $X_2$<li>Fit $Y \sim \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2$<li>Formula for confidence interval is same as simple model<li>Center predictors for moderation<ul><li>Easier to interpret<li>Reduces redundancy between $X_1$ and $X_1 X_2$ but does not change confidence interval of $\beta_3$, as long as we have simple parameters ($\beta_1$ and $\beta_2$)<ul><li>This is true of any linear change to parameters</ul></ul></ul><p>Mediation (cf <a href="https://sci-hub.bz/http://www.annualreviews.org/doi/abs/10.1146/annurev.psych.58.110405.085542">Mediation Analysis</a>):<ul><li>Want to separate direct effect of $X_1$ on $Y$ vs indirect effect via effect on $X_2$<li>Fit <script type="math/tex">M = i_1 + aX + e_1\\ Y = i_2 + cX + e_2\\ Y = i_3 + dX + bM + e_3</script><li>Casual steps procedure<ul><li>Test a is significant vs null<li>Test c is significant vs null<li>Test b is significant vs without b<li>Test d is not significant vs without d<li>Often low power</ul><li>Sobel test:<ul><li>Test $Z = ab \sim Normal$<li>$Z \sim Normal$ is often a poor approximation - use simulation instead</ul><li><a href="https://en.wikipedia.org/wiki/Structural_equation_modeling">Structural Equation Modeling</a></ul><p><strong>Caution - <a href="http://www2.psych.ubc.ca/~schaller/528Readings/BullockGreenHa2010.pdf">Don’t Expect An Easy Answer</a></strong><p><a href="/classes/stats/Week5.ipynb">Exercises</a><h2 id="lecture-6"><a href="https://moodle.ucl.ac.uk/course/view.php?id=11131&amp;section=6">Lecture 6</a></h2><p>ANOVA - analysis of variance - modeling differences between group means.<p>Null model = same means.<p>Contrast codes:<ul><li>Want to compare against a null-model where the parameters are restricted to some hyperplane, but analytic solution to GLM can only handle axis-aligned hyperplanes.<ul><li>Eg 2x2 control/diet x male/female. ‘Diet effect does not vary between male/female’ is equiv to ‘control/male - diet/male = control/female - diet/female’</ul><li>Solution: change to basis - $Y = A + BLX$<li>Rows of $L$ should be orthogonal<ul><li>Avoids introducing spurious correlations in transformed data, which would create correlations between confidence intervals<li>Allows interpreting as difference of means<ul><li>Even when cell sizes are unequal!<li>Otherwise null hypothesis is same but error is split differently across parameters</ul><li>Allows partitioning out $SSR$ due to each parameter (because SSR is linear function of group means)<ul><li>As long as cell sizes are equal - otherwise denominator of SSR is not same across rows</ul></ul><li>For given row $\lambda$, comparing against model without that parameter reduces to $\mathrm{SSR} = \frac{(\sum_k \lambda_k \bar{Y}_k) ^2}{\sum_k (\lambda_k^2 / n_k)}$<li>If a row sums to 0, parameter can be interpreted as difference of means (<a href="http://journals.sagepub.com/doi/full/10.1177/0013164416668950">source</a>).<li>Formula for confidence interval is same as simple model<li>To test for differences between means of $m$ groups, can use $m-1$ orthogonal rows<ul><li>Gives $b = \frac{\sum_k \lambda_k \bar{Y}_k}{\sum_k \lambda_k^2}$<li>(<strong>Means $L$ does not have rank n - can’t reconstruct original parameters - is this ok?</strong>)</ul></ul><p>With unequal cell sizes, orthogonal rows can still introduce redundancy (in generate case of only one datapoint Y=0, X+Y and X-Y are orthogonal but perfectly anti-correlated).<p>Helmert codes - $\lambda_{i,i} = m-i$ and $\forall j &gt; i \ldotp \lambda_{i,j} = -1$<p>Orthogonal polynomial codes:<ul><li>$Y$ as polynomial of category<li>Each row fits $b_n X^n - \text{previous rows}$<li>Differs from simply fitting a polynomial because based on group means rather than individual points - latter weights error towards larger categories</ul><p>Dummy codes - $\lambda_{i,i+1} = 1$ and $\lambda_{i,j} = 0$ otherwise. Not contrast codes - interpret $\lambda_i$ as comparing case $i$ vs case $0$.<p>Unequal cell sizes are weird, because mean of group means is not mean of individuals.<p>Multiple comparisons abound.<ul><li>In planned comparisons use $.05/m$<li>In post-hoc comparison use Scheffe adjusted critical value<ul><li>Fixes type 1 rate at $.05$<li>Any contrast exceeds critical value iff omnibus test is significant</ul></ul><p>For power analysis estimate:<p>\begin{align} \hat{\eta}^2 &amp;= 1 - \frac{(1 - \mathrm{PRE})(n - \mathrm{PC})}{n - \mathrm{PA}} \cr &amp;= \left( \frac{ m \sigma^2 (\sum \lambda^2 / n_k) }{ (\sum \lambda_k \mu_k)^2 } + 1 \right) ^{-1} \end{align}<p>where $\mu_k$ is predicted group mean and $\sigma^2$ is predicted within-cell variance.<ul><li>Power for omnibus test is maximized when all cell sizes are equal.<li>Power for contrast is maximized when cell sizes proportional to weights.</ul><p>With multiple categorical variables a good tactic is:<ul><li>Do contrast codes for decomposition<li>Map m-1 from each subspace into full space to ask basic questions<li>Take elementwise products of basic questions to ask about interactions</ul><p>Including useful variables often increases power for testing original variables, because reduces error which would obscure small effects.<p>Tukey-Kramer to test all possible pairs of groups.<h2 id="lecture-7"><a href="https://moodle.ucl.ac.uk/course/view.php?id=11131&amp;section=7">Lecture 7</a></h2><p>ANCOVA - analysis of covariance - same as ANOVA but with continuous as well as categorical predictors.<p>Typical use case - control vs treatment whilst controlling for covariate. Similar to before, can increase power by reducing error that is obscuring small effects.<p>Eg in pre/post test, typically more powerful than just modeling the difference. Latter effectively fixes the pre-test parameter to 1, so is only more powerful if ANCOVA estimate was close to 1.<p>Homogeneity of regression assumption = no interaction between categorical variable and continuous covariate.<h2 id="lecture-8"><a href="https://moodle.ucl.ac.uk/course/view.php?id=11131&amp;section=8">Lecture 8</a></h2><p>What if $e_i$ are not independent? Eg grouped or sequential data.<p>Repeated measures ANOVA - for grouped data, use weighted mean of group score.<p><strong>I can’t find a reason to prefer this over a hierarchical model.</strong><h2 id="lecture-9"><a href="https://moodle.ucl.ac.uk/course/view.php?id=11131&amp;section=9">Lecture 9</a></h2><p><strong>Losing interest in the course by this point. <a href="http://scattered-thoughts.net/blog/2017/06/28/notes-on-statistical-rethinking/">Statistical Rethinking</a> is much more useful.</strong><p>Multi-level models.<h2 id="lecture-10"><a href="https://moodle.ucl.ac.uk/course/view.php?id=11131&amp;section=11">Lecture 10</a></h2><p>Bayes factors.<p>Logistic regression.</article><footer><div>Questions? Comments? Just want to chat?</div><div><a href="mailto:jamie@scattered-thoughts.net>jamie@scattered-thoughts.net">jamie@scattered-thoughts.net</a></div><br><p><a href="/feed.xml"><img src="/img/rss.png"></img></a></footer></div>
